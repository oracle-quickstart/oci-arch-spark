## Copyright Â© 2020, Oracle and/or its affiliates. 
## All rights reserved. The Universal Permissive License (UPL), Version 1.0 as shown at http://oss.oracle.com/licenses/upl

title: "Create deployment of Apache Spark cluster in manager/worker mode on Oracle Cloud Infrastructure Compute instances"
stackDescription: "Deploy an Apache Spark cluster in manager/worker mode in Oracle Cloud Infrastructure."
schemaVersion: 1.1.0
version: "20190404"
locale: "en"

variableGroups:
- title: General Configuration
  visible: false
  variables:
  - tenancy_ocid
  - region

- title: Required Configuration
  visible: true  
  variables:
  - compartment_ocid
  - availablity_domain_name
  - master_instance_shape
  - worker_instance_shape
  - worker_node_count

- title: Optional Configuration
  visible: true
  variables:  
  - data_blocksize_in_gbs  
  - block_volumes_per_worker
  - enable_block_volumes
  - VCN_CIDR
  - vcn_dns_label
  - build_mode
  - use_hive
  - hadoop_version
  - useExistingVcn
  - hide_public_subnet
  - hide_private_subnet
  - myVcn
  - privateSubnet
  - publicSubnet
  - InstanceImageOCID
  - oci_service_gateway

variables:

  availablity_domain_name:
    type: oci:identity:availabilitydomain:name 
    title: "Availability Domain"
    description: "Select Availability Domain for your Apache Spark deployment"
    dependsOn:
      compartmentId: ${compartment_ocid}
    required: true

  compartment_ocid:
    type: oci:identity:compartment:id
    required: true
    visibile: true
    title: "Compartment"
    description: "Compartment where your Apache Spark will be deployed."

  region:
    type: oci:identity:region:name
    required: true
    visibile: false
    title: "Region"
    description: "Choose Region where your Apache Spark will be deployed."

  master_instance_shape:
    type: oci:core:instanceshape:name
    default: "VM.Standard2.1"
    title: "Master Node Shape"
    description: "Choose shape for Master Node"
    required: true
    dependsOn:
      compartmentId: ${compartment_ocid}

  worker_instance_shape:
    type: oci:core:instanceshape:name
    default: "VM.Standard2.1"
    title: "Spark Worker Nodes Shape"
    description: "Choose shape for Spark Worker Nodes."
    required: true
    dependsOn:
      compartmentId: ${compartment_ocid}

  worker_node_count:
    type: integer
    minimum: 1 
    title: "Number of Worker Nodes"
    description: "1 is the minimum requirement"
    required: true

  data_blocksize_in_gbs:
    type: integer
    title: "Worker Block Volume Size (GB)"
    description: "Choose Worker Block Volume Size (1 to 32,768GB)"
    minimum: 1
    maximum: 32768
    required: true

  block_volumes_per_worker:
    type: integer
    title: "Number of Block Volumes per Worker"
    description: "Choose Number of Block Volumes per Worker (0 to 32)"
    default: 1
    minimum: 0
    maximum: 32
    required: true

  enable_block_volumes:
    type: boolean
    title: "Enable Block Volumes"
    default: false
    description: "Enable additonal Block Volume capacity on Spark Workers"

  VCN_CIDR:
    type: string
    pattern: "^(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9]).(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9]).(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9]).(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\/(3[0-2]|[1-2]?[0-9])$"
    title: "VCN CIDR"
    description: "Choose VCN's CIDR block where Redis compute instances will be deployed."

  vcn_dns_label:
    type: string
    title: "VCN DNS Label"
    description: "Set the VCN DNS label to be used when creating VCN."

  build_mode:
    type: enum
    title: "Spark Build Mode Options"
    enum:
      - "Stand Alone"
      - "Hadoop"
      - "Mesos"
      - "Kubernetes"
    default: "Stand Alone"
    description: "Pick which build integration option you want." 

  use_hive:
    type: boolean
    title: "Hive/JCBC Integration"
    default: false
    description: "Check to enable Hive/JDBC integration"

  hadoop_version:
    type: enum
    title: "Hadoop Version"
    enum:
      - "2.6.x"
      - "2.7.x"
    default: "2.7.x"
    description: "Pick which Hadoop version to support"

  useExistingVcn:
    type: boolean
    title: "Already existing VCN"
    visible: false
    required: false

  hide_public_subnet:
    type: boolean
    title: "Hide Public subnet"
    visible: false
    required: false

  hide_private_subnet:
    type: boolean
    title: "Hide Private subnet"
    visible: false
    required: false

  myVcn:
    type: string
    title: "My Already existing VCN"
    visible: false
    required: false

  privateSubnet:
    type: string
    title: "Private Subnet"
    visible: false
    required: false

  publicSubnet:
    type: string
    title: "Public Subnet"
    visible: false
    required: false

  InstanceImageOCID:
    type: string
    title: "InstanceImageOCID"
    visible: false
    required: false

  oci_service_gateway:
    type: string
    title: "oci_service_gateway"
    visible: false
    required: false

outputs:

  spark_manager_url:
    title: "Spark Manager URL"
    displayText: "Spark Manager URL"
    type: copyableString
    visible: true

  generated_ssh_private_key:
    title: "Generated SSH Private Key"
    displayText: "Generated SSH Private Key"
    type: copyableString
    visible: true

